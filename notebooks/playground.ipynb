{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_days(time_str):\n",
    "    try:\n",
    "        # Extract numeric values from the string\n",
    "        times = [float(t) for t in time_str.split() if t.replace('.', '').isdigit()]\n",
    "        \n",
    "        # Convert to days based on the presence of \"week\" or \"month\"\n",
    "        if \"week\" in time_str:\n",
    "            times = [t * 7 for t in times]\n",
    "        elif \"month\" in time_str:\n",
    "            times = [t * 30 for t in times]  # Assuming 30 days per month\n",
    "        \n",
    "        # Return average if multiple values are present, else return the single value\n",
    "        return sum(times) / len(times) if times else None\n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Input Output Sentences for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Output (Days)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am storing fruit cocktail in my pantry at 60...</td>\n",
       "      <td>540.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input  Output (Days)\n",
       "0  I am storing fruit cocktail in my pantry at 60...          540.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting relevant columns related to expiration time for different locations\n",
    "relevant_columns_with_temp = [\n",
    "    (\"Pantry_Text\", \"pantry\", 60),\n",
    "    (\"Refrigerate_Text\", \"refrigerator\", 40),\n",
    "    (\"DateOfPurchase_Freeze_Text\", \"freezer\", 0),\n",
    "    # Add more columns as per requirements\n",
    "]\n",
    "\n",
    "# Initialize lists to hold the input and output sentences\n",
    "input_sentences_with_temp = []\n",
    "output_days_with_temp = []\n",
    "\n",
    "# Loop through the data\n",
    "for _, row in data.iterrows():\n",
    "    for col, location, temp in relevant_columns_with_temp:\n",
    "        # Check if expiration data is available\n",
    "        if pd.notna(row[col]):\n",
    "            # Construct the input sentence with temperature\n",
    "            input_sentence = f\"I am storing {row['Name']} in my {location} at {temp}°F, how long until it spoils or goes bad?\"\n",
    "            # Convert the output to days\n",
    "            output_day = convert_to_days(row[col])\n",
    "            \n",
    "            # Append to respective lists if output_day is not None\n",
    "            if output_day is not None:\n",
    "                input_sentences_with_temp.append(input_sentence)\n",
    "                output_days_with_temp.append(output_day)\n",
    "\n",
    "# Display first few input-output pairs with temperature consideration\n",
    "sample_data_days_with_temp = pd.DataFrame({\n",
    "    \"Input\": input_sentences_with_temp[:5],\n",
    "    \"Output (Days)\": output_days_with_temp[:5]\n",
    "})\n",
    "sample_data_days_with_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/m.bermejo/share/miniconda3/envs/pantry/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 12.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 115MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 480/480 [00:00<00:00, 4.92MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilRoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Tokenize the input sentences\n",
    "# Restrict the max length to 64 for computational efficiency; adjust as needed\n",
    "max_length = 64\n",
    "input_encodings = tokenizer(input_sentences_with_temp, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# Extract input ids and attention masks as PyTorch tensors\n",
    "input_ids = input_encodings['input_ids']\n",
    "attention_masks = input_encodings['attention_mask']\n",
    "\n",
    "# Convert output (days) to PyTorch tensor\n",
    "output_days_tensor = torch.tensor(output_days_with_temp).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Shape: torch.Size([1, 64])\n",
      "Attention Masks Shape: torch.Size([1, 64])\n",
      "Output Days Tensor Shape: torch.Size([1, 1])\n",
      "\n",
      "Original Sentence: I am storing fruit cocktail in my pantry at 60°F, how long until it spoils or goes bad?\n",
      "Decoded Sentence: I am storing fruit cocktail in my pantry at 60°F, how long until it spoils or goes bad?\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"Input IDs Shape:\", input_ids.shape)\n",
    "print(\"Attention Masks Shape:\", attention_masks.shape)\n",
    "print(\"Output Days Tensor Shape:\", output_days_tensor.shape)\n",
    "\n",
    "# Decode a tokenized sentence back to text\n",
    "decoded_sentence = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Display original and decoded sentences\n",
    "print(\"\\nOriginal Sentence:\", input_sentences_with_temp[0])\n",
    "print(\"Decoded Sentence:\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 5.18MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load configuration and create model\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "model = DistilBertForSequenceClassification(config).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, output_days_tensor)\n",
    "print(len(dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute explicit sizes for training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Ensure non-zero sizes\n",
    "if train_size == 0 or val_size == 0:\n",
    "    raise ValueError(\"Insufficient data for splitting into training and validation sets.\")\n",
    "\n",
    "# Proceed with splitting and data loader creation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pantry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
